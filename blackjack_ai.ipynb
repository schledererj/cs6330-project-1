{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS6330 Project 1: Reinforcement Learning Blackjack Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import blackjack\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this project is to build an intelligent Blackjack player using reinforcement (q-) learning.\n",
    "\n",
    "## Control experiment\n",
    "\n",
    "For a control experiment, we'll build a Blackjack player who, per basic Blackjack strategy, hits up to 17, then stands. The dealer will also play this way. This is a _simplified_ version of Blackjack, where we'll treat the deck as a \"continuous shuffle\" shoe, meaning that each card dealt is a random choice from 52 cards, meaning that the same card twice in a row is possible, but very improbable (though it would be an interesting future experiment to observe how the policy changes should the game use 1-, 2-, 4-deck shoes (and so on) with shuffling taking place when the deck is exhausted). We'll simulate 1,000 hands and see how the player performs. Check the accompanying `blackjack.py` file for the implementations of the player, the scoring algorithm, and more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "games = list()\n",
    "\n",
    "for x in range(0, 1000):\n",
    "    games.append(blackjack.Game().play_hand())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(games)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(df['winner'])\n",
    "plt.title('Distribution of hand winners for 1000 hands; first pass')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dealer_win = df['winner'].value_counts()['dealer'] / 1000. * 100\n",
    "player_win = df['winner'].value_counts()['player'] / 1000. * 100\n",
    "print(f\"Player win percentage: {(player_win):.2f}%\")\n",
    "print(f\"Dealer win percentage: {(dealer_win):.2f}%\")\n",
    "print(f\"House edge: {(dealer_win - 50):.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measuring the house edge\n",
    "\n",
    "As a casino game, the rules of Blackjack set up to give the dealer (the _house_) an advantage (an _edge_). Meaning that, after enough games, we'll observe the house averaging out to win a certain percentage over 50%, but never under (hence why gambling is always a bad decision). In a real game of Blackjack, i.e with good player strategy, splits, double-downs, 3:2 payouts on natural Blackjacks, etc, this can be well under 5%, but for this first pass, we got 8.6%. Let's measure the house edge in the following experiments to see if a player whose moves are governed by a Q-learning policy can learn to play better, quantitatively measured by a reduction in the house edge.\n",
    "\n",
    "## Q-learning experiment\n",
    "\n",
    "For the next experiment, we'll use a Q-learning algorithm to train a policy which we'll pass to the player and run the same experiment as before. For the first round of training, we're not taking into account the dealer's score, and will only update the Q-table based on whether the player stands before 21, hits exactly 21, or busts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = blackjack.QLearningTrainer()\n",
    "trainer.optimize_q_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = trainer.compile_policy_from_trained_q_table()\n",
    "games = list()\n",
    "for x in range(0, 1000):\n",
    "    games.append(blackjack.Game(policy).play_hand())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(games)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(df['winner'])\n",
    "plt.title('Distribution of hand winners for 1000 hands; second pass')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dealer_win = df['winner'].value_counts()['dealer'] / 1000. * 100\n",
    "player_win = df['winner'].value_counts()['player'] / 1000. * 100\n",
    "print(f\"Player win percentage: {(player_win):.2f}%\")\n",
    "print(f\"Dealer win percentage: {(dealer_win):.2f}%\")\n",
    "print(f\"House edge: {(dealer_win - 50):.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that this method slightly weakened the player and strengthened the house edge. But, that could be because we're not taking into account the dealer's score, so we don't know if we truly win unless we hit exactly 21.\n",
    "\n",
    "## Q-learning experiment 2\n",
    "\n",
    "Let's switch the positions of the dealer and the player, so we'll know the dealer's score before the player starts. With that data, we can build a different rewards table that considers the dealer's score, and therefore, whether or not we won not by hitting 21, but rather simply having more points than the dealer, which is a much more common scenario we weren't counting for before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = blackjack.QLearningTrainer()\n",
    "trainer.optimize_q_table_v2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = trainer.compile_policy_from_trained_q_table()\n",
    "policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a policy much more similar to the standard Blackjack strategy! Let's run the 1,000-game experiment again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "games = list()\n",
    "for x in range(0, 1000):\n",
    "    games.append(blackjack.Game(policy).play_hand())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(games)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(df['winner'])\n",
    "plt.title('Distribution of hand winners for 1000 hands; third pass')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dealer_win = df['winner'].value_counts()['dealer'] / 1000. * 100\n",
    "player_win = df['winner'].value_counts()['player'] / 1000. * 100\n",
    "print(f\"Player win percentage: {(player_win):.2f}%\")\n",
    "print(f\"Dealer win percentage: {(dealer_win):.2f}%\")\n",
    "print(f\"House edge: {(dealer_win - 50):.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "It seems that this is around the same house edge as the previous iteration. However, this isn't the furthest we could take the experiments. It would be interesting to see if the house edge could be improved by grid-searching for better alpha, lambda, and epsilon values. Futhermore, we could implement other common rules for blackjack for the Q-learning player, such as splitting, doubling-down, and a more realistic deck, as in using single- or double-decks with shuffling when the deck depletes."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b9390f69085e7fdca2606005e4a65a3191529f88127ab1636ced00a1039e631b"
  },
  "kernelspec": {
   "display_name": "Python 3.10.2 ('.env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
